# -*- coding: utf-8 -*-
"""hand-ges-reco-bi-lstm (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yP5au9wyFbNCqb-ckJNezGU-7iHS_JI6
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Bidirectional, LSTM, Reshape
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import cv2

# === CONFIG === #
IMG_SIZE = 64
BATCH_SIZE = 32
EPOCHS = 20
DATASET_PATH = '/kaggle/input/hand-ges/leapGestRecog/'  # <-- corrected path

#Function to create a "virtual dataset" with correct gesture grouping
def create_dataset_symlink(base_path='/kaggle/working/gesture_dataset_bilstm'):
    if not os.path.exists(base_path):
        os.makedirs(base_path)
        for person_folder in os.listdir(DATASET_PATH):
            person_path = os.path.join(DATASET_PATH, person_folder)
            if os.path.isdir(person_path):
                for gesture_folder in os.listdir(person_path):
                    gesture_path = os.path.join(person_path, gesture_folder)
                    if os.path.isdir(gesture_path):
                        target_folder = os.path.join(base_path, gesture_folder)
                        if not os.path.exists(target_folder):
                            os.makedirs(target_folder)
                        for img in os.listdir(gesture_path):
                            src_img = os.path.join(gesture_path, img)
                            dst_img = os.path.join(target_folder, f"{person_folder}_{img}")
                            os.symlink(src_img, dst_img)
    return base_path

# Build the symlink dataset
new_dataset_path = create_dataset_symlink()

datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

train_gen = datagen.flow_from_directory(
    new_dataset_path,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

val_gen = datagen.flow_from_directory(
    new_dataset_path,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=True
)

# === MODEL === #
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Reshape((14, 14 * 64)),  # (timesteps, features) for LSTM

    Bidirectional(LSTM(64, return_sequences=False)),

    Dropout(0.5),
    Dense(128, activation='relu'),
    Dense(train_gen.num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# === TRAINING === #
history = model.fit(
    train_gen,
    epochs=EPOCHS,
    validation_data=val_gen
)

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns

# === ACCURACY & LOSS PLOTS === #
plt.figure(figsize=(12, 5))

# Accuracy Plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

val_gen.reset()
Y_pred = model.predict(val_gen)
y_pred = np.argmax(Y_pred, axis=1)
y_true = val_gen.classes
class_labels = list(val_gen.class_indices.keys())

cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)
fig, ax = plt.subplots(figsize=(10, 10))
disp.plot(cmap=plt.cm.Blues, ax=ax, xticks_rotation=45)
plt.title("Confusion Matrix")
plt.show()

print("Classification Report:\n")
print(classification_report(y_true, y_pred, target_names=class_labels))

# === PREDICTION FUNCTION === #

def predict_gesture_bilstm(img_path):
    gray_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)

    img_resized = cv2.resize(gray_img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)

    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)

    img_array = img_rgb.astype('float32') / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    predictions = model.predict(img_array)
    class_idx = np.argmax(predictions[0])
    class_labels = list(train_gen.class_indices.keys())

    # Extract the proper gesture name
    predicted_label = '_'.join(class_labels[class_idx].split('_')[1:])  # handles cases like '08_palm_moved'

    print(f"Predicted gesture: {predicted_label}")

    display_img = cv2.resize(gray_img, (256, 256), interpolation=cv2.INTER_NEAREST)
    plt.figure(figsize=(6, 6), dpi=100)
    plt.imshow(display_img, cmap='gray', interpolation='nearest')
    plt.title(f"Prediction: {predicted_label}", fontsize=16)
    plt.axis('off')
    plt.show()

predict_gesture_bilstm('/kaggle/input/hand-ges/leapGestRecog/02/05_thumb/frame_02_05_0009.png')

predict_gesture_bilstm('/kaggle/input/hand-ges/leapGestRecog/06/03_fist/frame_06_03_0007.png')

predict_gesture_bilstm('/kaggle/input/hand-ges/leapGestRecog/07/10_down/frame_07_10_0019.png')

predict_gesture_bilstm('/kaggle/input/hand-ges/leapGestRecog/04/05_thumb/frame_04_05_0018.png')

predict_gesture_bilstm('/kaggle/input/hand-ges/leapGestRecog/03/08_palm_moved/frame_03_08_0015.png')