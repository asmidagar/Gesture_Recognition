# -*- coding: utf-8 -*-
"""hand-ges-reco-2dcnn (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OdkcHqC6mSYOgLrT5u3NVuWpXfbFUeND
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import cv2



IMG_SIZE = 64
BATCH_SIZE = 32
EPOCHS = 20
DATASET_PATH = '/kaggle/input/hand-gestures/leapGestRecog/'

def create_dataset_symlink(base_path='/kaggle/working/gesture_dataset'):
    if not os.path.exists(base_path):
        os.makedirs(base_path)
        for person_folder in os.listdir(DATASET_PATH):
            person_path = os.path.join(DATASET_PATH, person_folder)
            if os.path.isdir(person_path):
                for gesture_folder in os.listdir(person_path):
                    gesture_path = os.path.join(person_path, gesture_folder)
                    if os.path.isdir(gesture_path):
                        target_folder = os.path.join(base_path, gesture_folder)
                        if not os.path.exists(target_folder):
                            os.makedirs(target_folder)
                        for img in os.listdir(gesture_path):
                            src_img = os.path.join(gesture_path, img)
                            dst_img = os.path.join(target_folder, f"{person_folder}_{img}")
                            os.symlink(src_img, dst_img)
    return base_path

new_dataset_path = create_dataset_symlink()

datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

train_gen = datagen.flow_from_directory(
    new_dataset_path,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

val_gen = datagen.flow_from_directory(
    new_dataset_path,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=True
)

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(train_gen.num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

history = model.fit(
    train_gen,
    epochs=EPOCHS,
    validation_data=val_gen
)

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns

plt.figure(figsize=(12, 5))

# Accuracy Plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

val_gen.reset()
y_pred = model.predict(val_gen)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = val_gen.classes
class_labels = list(val_gen.class_indices.keys())

# Classification Report
print("Classification Report:\n")
print(classification_report(y_true, y_pred_classes, target_names=class_labels))

conf_matrix = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

def predict_gesture(img_path):
    original_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    resized_for_model = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)
    img_array = resized_for_model.astype('float32') / 255.0
    img_array = np.expand_dims(img_array, axis=-1)   # (64, 64, 1)
    img_array = np.repeat(img_array, 3, axis=-1)      # (64, 64, 3)
    img_array = np.expand_dims(img_array, axis=0)     # (1, 64, 64, 3)

    predictions = model.predict(img_array)
    class_idx = np.argmax(predictions[0])
    class_labels = list(train_gen.class_indices.keys())

    predicted_label = class_labels[class_idx].split('_')[1]  # Take 'palm', 'index', etc.

    print(f"Predicted gesture: {predicted_label.capitalize()}")  # capitalize for neatness

    plt.figure(figsize=(6, 6), dpi=120)
    plt.imshow(original_img, cmap='gray', interpolation='nearest')
    plt.title(f"Prediction: {predicted_label.capitalize()}", fontsize=16)
    plt.axis('off')
    plt.show()

# === TEST EXAMPLE === #
predict_gesture('/kaggle/input/hand-gestures/leapGestRecog/00/01_palm/frame_00_01_0013.png')  # Replace with your image path

predict_gesture('/kaggle/input/hand-gestures/leapGestRecog/00/06_index/frame_00_06_0023.png')  # Replace with your image path

predict_gesture('/kaggle/input/hand-gestures/leapGestRecog/00/10_down/frame_00_10_0019.png')  # Replace with your image path

predict_gesture('/kaggle/input/hand-gestures/leapGestRecog/00/07_ok/frame_00_07_0005.png')  # Replace with your image path

predict_gesture('/kaggle/input/hand-gestures/leapGestRecog/00/08_palm_moved/frame_00_08_0006.png')